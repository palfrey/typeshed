from lib2to3.pgen2.token import *  # noqa
from typing import Callable, Iterable, Iterator, Protocol

_Coord = tuple[int, int]
_TokenEater = Callable[[int, str, _Coord, _Coord, str], None]
_TokenInfo = tuple[int, str, _Coord, _Coord, str]

class TokenError(Exception): ...
class StopTokenizing(Exception): ...

class _Readline(Protocol):
    def __call__(self, __size: int | None = ...) -> str: ...

def tokenize(readline: _Readline, tokeneater: _TokenEater = ...) -> None: ...

class Untokenizer:
    tokens: list[str]
    prev_row: int
    prev_col: int
    def __init__(self) -> None: ...
    def add_whitespace(self, start: _Coord) -> None: ...
    def untokenize(self, iterable: Iterable[_TokenInfo]) -> str: ...
    def compat(self, token: tuple[int, str], iterable: Iterable[_TokenInfo]) -> None: ...

def untokenize(iterable: Iterable[_TokenInfo]) -> str: ...
def generate_tokens(readline: _Readline) -> Iterator[_TokenInfo]: ...
